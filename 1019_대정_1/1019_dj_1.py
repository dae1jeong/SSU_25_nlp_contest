# -*- coding: utf-8 -*-
"""1019_dj_1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mDporr1siPbwV4w1WvIudkdhXi-N3yAS
"""

import os
import pandas as pd
from sklearn.metrics import accuracy_score
import numpy as np
import warnings

# 경고 메시지 무시 설정
warnings.filterwarnings("ignore")

# 필요한 라이브러리 임포트
# BertTokenizer 임포트 유지
from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer, BertTokenizer
from torch.utils.data import Dataset
import torch

# ==============================================================================
# 1. 데이터 로드 및 환경 설정
# ==============================================================================

# A. Google Drive 마운트
from google.colab import drive
drive.mount('/content/drive')

# B. 데이터 파일 경로 설정
DATA_PATH = "/content/drive/MyDrive/nlp_contest"

# C. 파일 로드 (train, valid, test)
try:
    df_train = pd.read_csv(os.path.join(DATA_PATH, 'train.csv'))
    df_valid = pd.read_csv(os.path.join(DATA_PATH, 'valid.csv'))
    df_test = pd.read_csv(os.path.join(DATA_PATH, 'test.csv'))
    print("✅ 데이터 로드 완료 (Train: {}, Valid: {}, Test: {})".format(len(df_train), len(df_valid), len(df_test)))
except FileNotFoundError:
    print("❌ 경로 '{}'에서 파일을 찾을 수 없습니다. DATA_PATH를 확인해주세요.".format(DATA_PATH))
    raise

# D. 모델 정의 및 하이퍼파라미터 설정
MODEL_NAME = "monologg/kobert"
MAX_LEN = 128
BATCH_SIZE = 32
NUM_EPOCHS = 1           # 🚨 빠른 테스트를 위해 1 Epoch로 유지
LEARNING_RATE = 3e-5

# ==============================================================================
# 2. 전처리 및 특징 추출 (BERT-Style Tokenization)
# ==============================================================================

# A. 토크나이저 로드
tokenizer = BertTokenizer.from_pretrained(MODEL_NAME)

# B. PyTorch Dataset 클래스 정의
class SentimentDataset(Dataset):
    def __init__(self, texts, labels=None):
        self.texts = texts
        self.labels = labels

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        encoding = tokenizer(
            str(self.texts[idx]),
            truncation=True,
            padding='max_length',
            max_length=MAX_LEN,
            return_tensors='pt'
        )

        item = {key: val.squeeze() for key, val in encoding.items()}

        if self.labels is not None:
            item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long)

        return item

# C. Dataset 객체 생성
train_dataset = SentimentDataset(df_train['text'].tolist(), df_train['label'].tolist())
valid_dataset = SentimentDataset(df_valid['text'].tolist(), df_valid['label'].tolist())
test_texts = df_test['text'].tolist()
test_dataset = SentimentDataset(test_texts)

print("✅ Dataset 객체 생성 완료")

# ==============================================================================
# 3. 모델 정의 및 학습 (Fine-tuning)
# ==============================================================================

# A. 모델 로드
model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=2)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)
print("✅ KoBERT 모델 로드 및 GPU/CPU 할당 완료")

# B. 평가 지표 함수 정의 (Accuracy) - Sklearn 사용
def compute_metrics(p):
    # 이 테스트 실행에서는 사용되지 않지만, Trainer 객체 생성을 위해 필요합니다.
    preds = np.argmax(p.predictions, axis=1)
    return {"accuracy": accuracy_score(p.label_ids, preds)}

# C. TrainingArguments 설정
training_args = TrainingArguments(
    output_dir='./results',
    num_train_epochs=NUM_EPOCHS,               # 🚨 1 Epoch
    per_device_train_batch_size=BATCH_SIZE,
    per_device_eval_batch_size=BATCH_SIZE,
    warmup_steps=500,
    weight_decay=0.01,
    logging_dir='./logs',
    logging_steps=500,
    learning_rate=LEARNING_RATE,
    # 🚨 오류를 피하기 위해 평가/저장 전략 관련 파라미터는 모두 제거했습니다.
    report_to="none"
)


# D. Trainer 객체 생성 및 학습 시작
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=valid_dataset,
    compute_metrics=compute_metrics,
)

print("🚀 모델 학습 시작...")
trainer.train()
print("✅ 모델 학습 완료")

# ==============================================================================
# 4. 예측 및 제출 파일 생성
# ==============================================================================

# A. 테스트 데이터 예측
print("➡️ 테스트 데이터 예측 시작...")
predictions = trainer.predict(test_dataset)

# B. 예측 결과 (logits)를 라벨 (0 또는 1)로 변환
predicted_labels = np.argmax(predictions.predictions, axis=1)

# C. submission.csv 파일 형식 생성
submission = pd.DataFrame({
    'id': df_test['id'],
    'label': predicted_labels
})

# D. submission.csv 파일 저장
SUBMISSION_PATH = os.path.join(DATA_PATH, 'submission.csv')
submission.to_csv(SUBMISSION_PATH, index=False)

print("\n🎉 최종 예측 완료 및 submission.csv 파일 생성!")
print("파일 경로: {}".format(SUBMISSION_PATH))
print("제출 형식 확인 (상위 5개):")
print(submission.head())