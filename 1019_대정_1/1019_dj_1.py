# -*- coding: utf-8 -*-
"""1019_dj_1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mDporr1siPbwV4w1WvIudkdhXi-N3yAS
"""

import os
import pandas as pd
from sklearn.metrics import accuracy_score
import numpy as np
import warnings

# ê²½ê³  ë©”ì‹œì§€ ë¬´ì‹œ ì„¤ì •
warnings.filterwarnings("ignore")

# í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸
# BertTokenizer ì„í¬íŠ¸ ìœ ì§€
from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer, BertTokenizer
from torch.utils.data import Dataset
import torch

# ==============================================================================
# 1. ë°ì´í„° ë¡œë“œ ë° í™˜ê²½ ì„¤ì •
# ==============================================================================

# A. Google Drive ë§ˆìš´íŠ¸
from google.colab import drive
drive.mount('/content/drive')

# B. ë°ì´í„° íŒŒì¼ ê²½ë¡œ ì„¤ì •
DATA_PATH = "/content/drive/MyDrive/nlp_contest"

# C. íŒŒì¼ ë¡œë“œ (train, valid, test)
try:
    df_train = pd.read_csv(os.path.join(DATA_PATH, 'train.csv'))
    df_valid = pd.read_csv(os.path.join(DATA_PATH, 'valid.csv'))
    df_test = pd.read_csv(os.path.join(DATA_PATH, 'test.csv'))
    print("âœ… ë°ì´í„° ë¡œë“œ ì™„ë£Œ (Train: {}, Valid: {}, Test: {})".format(len(df_train), len(df_valid), len(df_test)))
except FileNotFoundError:
    print("âŒ ê²½ë¡œ '{}'ì—ì„œ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. DATA_PATHë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.".format(DATA_PATH))
    raise

# D. ëª¨ë¸ ì •ì˜ ë° í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì •
MODEL_NAME = "monologg/kobert"
MAX_LEN = 128
BATCH_SIZE = 32
NUM_EPOCHS = 1           # ğŸš¨ ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•´ 1 Epochë¡œ ìœ ì§€
LEARNING_RATE = 3e-5

# ==============================================================================
# 2. ì „ì²˜ë¦¬ ë° íŠ¹ì§• ì¶”ì¶œ (BERT-Style Tokenization)
# ==============================================================================

# A. í† í¬ë‚˜ì´ì € ë¡œë“œ
tokenizer = BertTokenizer.from_pretrained(MODEL_NAME)

# B. PyTorch Dataset í´ë˜ìŠ¤ ì •ì˜
class SentimentDataset(Dataset):
    def __init__(self, texts, labels=None):
        self.texts = texts
        self.labels = labels

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        encoding = tokenizer(
            str(self.texts[idx]),
            truncation=True,
            padding='max_length',
            max_length=MAX_LEN,
            return_tensors='pt'
        )

        item = {key: val.squeeze() for key, val in encoding.items()}

        if self.labels is not None:
            item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long)

        return item

# C. Dataset ê°ì²´ ìƒì„±
train_dataset = SentimentDataset(df_train['text'].tolist(), df_train['label'].tolist())
valid_dataset = SentimentDataset(df_valid['text'].tolist(), df_valid['label'].tolist())
test_texts = df_test['text'].tolist()
test_dataset = SentimentDataset(test_texts)

print("âœ… Dataset ê°ì²´ ìƒì„± ì™„ë£Œ")

# ==============================================================================
# 3. ëª¨ë¸ ì •ì˜ ë° í•™ìŠµ (Fine-tuning)
# ==============================================================================

# A. ëª¨ë¸ ë¡œë“œ
model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=2)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)
print("âœ… KoBERT ëª¨ë¸ ë¡œë“œ ë° GPU/CPU í• ë‹¹ ì™„ë£Œ")

# B. í‰ê°€ ì§€í‘œ í•¨ìˆ˜ ì •ì˜ (Accuracy) - Sklearn ì‚¬ìš©
def compute_metrics(p):
    # ì´ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ì—ì„œëŠ” ì‚¬ìš©ë˜ì§€ ì•Šì§€ë§Œ, Trainer ê°ì²´ ìƒì„±ì„ ìœ„í•´ í•„ìš”í•©ë‹ˆë‹¤.
    preds = np.argmax(p.predictions, axis=1)
    return {"accuracy": accuracy_score(p.label_ids, preds)}

# C. TrainingArguments ì„¤ì •
training_args = TrainingArguments(
    output_dir='./results',
    num_train_epochs=NUM_EPOCHS,               # ğŸš¨ 1 Epoch
    per_device_train_batch_size=BATCH_SIZE,
    per_device_eval_batch_size=BATCH_SIZE,
    warmup_steps=500,
    weight_decay=0.01,
    logging_dir='./logs',
    logging_steps=500,
    learning_rate=LEARNING_RATE,
    # ğŸš¨ ì˜¤ë¥˜ë¥¼ í”¼í•˜ê¸° ìœ„í•´ í‰ê°€/ì €ì¥ ì „ëµ ê´€ë ¨ íŒŒë¼ë¯¸í„°ëŠ” ëª¨ë‘ ì œê±°í–ˆìŠµë‹ˆë‹¤.
    report_to="none"
)


# D. Trainer ê°ì²´ ìƒì„± ë° í•™ìŠµ ì‹œì‘
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=valid_dataset,
    compute_metrics=compute_metrics,
)

print("ğŸš€ ëª¨ë¸ í•™ìŠµ ì‹œì‘...")
trainer.train()
print("âœ… ëª¨ë¸ í•™ìŠµ ì™„ë£Œ")

# ==============================================================================
# 4. ì˜ˆì¸¡ ë° ì œì¶œ íŒŒì¼ ìƒì„±
# ==============================================================================

# A. í…ŒìŠ¤íŠ¸ ë°ì´í„° ì˜ˆì¸¡
print("â¡ï¸ í…ŒìŠ¤íŠ¸ ë°ì´í„° ì˜ˆì¸¡ ì‹œì‘...")
predictions = trainer.predict(test_dataset)

# B. ì˜ˆì¸¡ ê²°ê³¼ (logits)ë¥¼ ë¼ë²¨ (0 ë˜ëŠ” 1)ë¡œ ë³€í™˜
predicted_labels = np.argmax(predictions.predictions, axis=1)

# C. submission.csv íŒŒì¼ í˜•ì‹ ìƒì„±
submission = pd.DataFrame({
    'id': df_test['id'],
    'label': predicted_labels
})

# D. submission.csv íŒŒì¼ ì €ì¥
SUBMISSION_PATH = os.path.join(DATA_PATH, 'submission.csv')
submission.to_csv(SUBMISSION_PATH, index=False)

print("\nğŸ‰ ìµœì¢… ì˜ˆì¸¡ ì™„ë£Œ ë° submission.csv íŒŒì¼ ìƒì„±!")
print("íŒŒì¼ ê²½ë¡œ: {}".format(SUBMISSION_PATH))
print("ì œì¶œ í˜•ì‹ í™•ì¸ (ìƒìœ„ 5ê°œ):")
print(submission.head())