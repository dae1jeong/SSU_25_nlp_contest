# -*- coding: utf-8 -*-
"""메로롱

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/15HE9gksLuo4OrriprTz-J4pfel7PYiZU
"""

!pip install konlpy JPype1

# =======================================
# TF-IDF + Logistic Regression
# train.csv / valid.csv / test.csv
# submission.csv (id,label)
# =======================================

# !pip install -q scikit-learn pandas konlpy JPype1

import pandas as pd, re
from konlpy.tag import Okt
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.pipeline import Pipeline
from sklearn.metrics import classification_report, accuracy_score, f1_score

DATA_DIR = "/content/drive/MyDrive/Colab Notebooks/자연어_대회"
TRAIN = f"{DATA_DIR}/train-2.csv"
VALID = f"{DATA_DIR}/valid.csv"
TEST  = f"{DATA_DIR}/test.csv"
SUB   = f"{DATA_DIR}/submission.csv"

# ---------- 1) 컬럼 정규화 ----------
def normalize_columns(df):
    df.columns = (
        df.columns
          .str.replace("\ufeff", "", regex=False)  # BOM 제거
          .str.strip()
          .str.lower()
    )
    rename_map = {
        "labels": "label",
        "target": "label",
        "sentiment": "label",
        "document": "text",
        "review": "text",
    }
    df.rename(columns={k: v for k, v in rename_map.items() if k in df.columns}, inplace=True)
    return df

# ---------- 2) 전처리 ----------
def clean_text(t):
    if not isinstance(t, str):
        t = "" if pd.isna(t) else str(t)
    t = re.sub(r"[^가-힣ㄱ-ㅎㅏ-ㅣa-zA-Z0-9\s]", " ", t)
    return re.sub(r"\s+", " ", t).strip()

okt = Okt()
stopwords = ['은','는','이','가','을','를','에','도','으로','그리고','하지만']

def tokenize(t): return okt.morphs(t)
def preprocess(df):
    df["clean_text"] = df["text"].apply(clean_text)
    df["tokens"] = df["clean_text"].apply(tokenize)
    df["tokens"] = df["tokens"].apply(lambda x: [w for w in x if w not in stopwords])
    df["processed_text"] = df["tokens"].apply(lambda x: " ".join(x))
    return df

# ---------- 3) 데이터 로드 ----------
train = normalize_columns(pd.read_csv(TRAIN))
valid = normalize_columns(pd.read_csv(VALID))
test  = normalize_columns(pd.read_csv(TEST))

# ---------- 4) 전처리 ----------
train = preprocess(train)
valid = preprocess(valid)
test  = preprocess(test)

# ---------- 5) TF-IDF + Logistic Regression ----------
pipe = Pipeline([
    ("tfidf", TfidfVectorizer(
        max_features=50000,
        ngram_range=(1, 2),
        min_df=2,
        max_df=0.9
    )),
    ("clf", LogisticRegression(
        C=2.0,
        class_weight="balanced",
        solver="liblinear",
        random_state=42
    ))
])

pipe.fit(train["processed_text"], train["label"].astype(int))

# ---------- 6) VALID 평가 ----------
pred_val = pipe.predict(valid["processed_text"])
print("\n[VALID 결과]")
print("Accuracy:", accuracy_score(valid["label"], pred_val))
print("F1 (macro):", f1_score(valid["label"], pred_val, average="macro"))
print(classification_report(valid["label"], pred_val, digits=4))

# ---------- 7) TEST 예측 → 제출 파일 저장 ----------
pred_test = pipe.predict(test["processed_text"]).astype(int)

# 제출 파일 형식: id,label
submission = pd.DataFrame({
    "id": test["id"],
    "label": pred_test
})
submission.to_csv(SUB, index=False)

print(f"\n✅ submission.csv 저장 완료: {SUB}")
print(submission.head())

